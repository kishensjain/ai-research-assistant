---
title: Distill AI Research Assistant
emoji: ğŸ”
colorFrom: blue
colorTo: indigo
sdk: gradio
sdk_version: "6.6.0"
app_file: main.py
pinned: false
---

# Distill ğŸ”

> Chat with anything. URLs, PDFs, Word docs, and YouTube videos â€” all in one place.

Distill is a lightweight **semantic RAG (Retrieval-Augmented Generation)** research assistant that lets you load multiple sources and have grounded conversations with them.

Paste a webpage url, upload a document, or drop a YouTube link â€” then ask questions and get answers backed by your content.

ğŸ”— **Live Demo:**  
https://kishensjain-distill-ai-research-assistant.hf.space/

---

## âœ¨ Features

- ğŸŒ **URL ingestion** â€” paste any webpage and chat with its content  
- ğŸ“„ **PDF & Word support** â€” upload `.pdf` and `.docx` files  
- ğŸ¥ **YouTube transcripts** â€” paste a link and chat with the video (via Supadata API)  
- ğŸ” **Semantic search** â€” retrieval powered by vector embeddings (not keyword matching)  
- ğŸ§  **Smart chunking** â€” content split into overlapping chunks (~1000 chars)  
- âš¡ **Streaming responses** â€” real-time answer generation  
- ğŸ“ **Auto summarization** â€” sources summarized when loaded  
- ğŸ’¬ **Multi-turn memory** â€” conversation history preserved  
- ğŸ–¥ï¸ **Gradio web UI** â€” clean browser-based interface

---

## ğŸ— Architecture

Distill implements semantic RAG from scratch â€” without any external RAG framework.

**Pipeline:**

1. **Ingest** â€” Fetch and clean content from URLs, PDFs, Word docs, or YouTube transcripts  
2. **Chunk** â€” Split content into overlapping text chunks  
3. **Embed** â€” Convert chunks into vector embeddings using `gemini-embedding-001`  
4. **Retrieve** â€” Compute cosine similarity between query and stored embeddings  
5. **Generate** â€” Send top-K relevant chunks + conversation history to the LLM  
6. **Stream** â€” Return the response in real time  

---

## Tech Stack

- **Gradio** â€” Web UI  
- **Gemini API** â€” Embeddings (`gemini-embedding-001`)  
- **Ollama Cloud** â€” LLM generation (OpenAI-compatible client)  
- **NumPy** â€” Cosine similarity computation  
- **BeautifulSoup4** â€” Web scraping  
- **pypdf** â€” PDF extraction  
- **python-docx** â€” Word document extraction  
- **Supadata API** â€” YouTube transcripts  
- **uv** â€” Package management  

---

## Getting Started

### 1. Clone the repo

```bash
git clone https://github.com/yourusername/distill-AI-research-assistant.git
cd distill-AI-research-assistant
```

### 2. Install dependencies(make sure you have uv installed)

```bash
uv sync
```

### 3. Set up your API key

Create a `.env` file in the root:

```
OLLAMA_API_KEY=your_key_here
SUPADATA_API_KEY=your_key_here
GEMINI_API_KEY=your_key_here
```

Or if using a different provider, update the client in `src/ui.py` accordingly.

### 4. Run the app

```bash
uv run main.py
```


---

## Project Structure

```
distill/
â”œâ”€â”€ main.py          # Entry point
â”œâ”€â”€ src/
    |â”€â”€ cosine_similarity.py # Cosine similarity
â”‚   â”œâ”€â”€ ingestion.py # URL, file, and YouTube loading
â”‚   â”œâ”€â”€ chunker.py   # Text splitting and relevance scoring
â”‚   â””â”€â”€ ui.py        # Gradio interface and LLM chat logic
â”œâ”€â”€ .env             # API keys (not committed)
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## How It Works

1. **Ingest** â€” content is fetched and cleaned from your sources
2. **Chunk** â€” content is split into overlapping chunks of ~1000 characters
3. **Retrieve** â€” when you ask a question, the most relevant chunks are selected using keyword matching
4. **Generate** â€” selected chunks are sent to the LLM along with your question and conversation history
5. **Stream** â€” the response streams back in real time

This is a lightweight implementation of **RAG (Retrieval-Augmented Generation)** built from scratch without any RAG framework.

---

## License

MIT